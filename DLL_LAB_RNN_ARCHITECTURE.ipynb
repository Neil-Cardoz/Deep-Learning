{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoqhjbtCfCpZJhAZ8cNMau",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neil-Cardoz/Deep-Learning/blob/main/DLL_LAB_RNN_ARCHITECTURE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals of RNN\n",
        "\n",
        "A Recurrent Neural Network (RNN) is designed to handle sequential data by using a hidden state that carries information across time steps.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "1. Study the fundamentals of Recurrent Neural Networks (RNNs).  \n",
        "2. Understand how hidden states store information across time steps.  \n",
        "3. Implement a simple RNN from scratch using NumPy.  \n",
        "4. Learn forward propagation through sequences.  \n",
        "5. Learn backward propagation through time (BPTT) for training.  \n",
        "6. Understand how gradients update weights and biases in RNNs."
      ],
      "metadata": {
        "id": "st-AHFjR1mva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (RNNs)\n",
        "\n",
        "**Recurrent Neural Networks (RNNs)** are a class of neural networks designed to work with **sequential data**, such as time-series, text, or speech. Unlike feedforward networks, RNNs **maintain a hidden state** that acts as memory, allowing the network to capture dependencies across time steps.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "* **Hidden State (`h_t`)**\n",
        "  The hidden state stores information from previous inputs and is updated at each time step using the formula:\n",
        "\n",
        "  ```\n",
        "  h_t = tanh(Wxh * x_t + Whh * h_(t-1) + bh)\n",
        "  ```\n",
        "\n",
        "  * `x_t` = input at time t\n",
        "  * `h_(t-1)` = hidden state from previous time step\n",
        "  * `Wxh`, `Whh` = weights\n",
        "  * `bh` = bias\n",
        "\n",
        "* **Output (`y_t`)**\n",
        "  Computed from the hidden state:\n",
        "\n",
        "  ```\n",
        "  y_t = Why * h_t + by\n",
        "  ```\n",
        "\n",
        "* **Backpropagation Through Time (BPTT)**\n",
        "  Gradients are propagated backward through all time steps to update weights and biases. This allows the network to learn temporal patterns.\n",
        "\n",
        "* **One-Hot Encoding**\n",
        "  Inputs are often represented as one-hot vectors when dealing with discrete sequences (e.g., characters or words).\n",
        "\n",
        "## Advantages\n",
        "\n",
        "* Can model temporal dependencies\n",
        "* Suitable for sequential tasks like text generation, speech recognition, and time-series forecasting\n",
        "\n",
        "## Limitations\n",
        "\n",
        "* Difficulty learning long-term dependencies due to **vanishing gradients**\n",
        "* Can be slow to train on long sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "B9Dikl0l2aFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Concepts in the Code\n",
        "\n",
        "* **Weights & Biases**\n",
        "\n",
        "  * `Wxh` – Input → Hidden\n",
        "  * `Whh` – Hidden → Hidden (recurrent)\n",
        "  * `Why` – Hidden → Output\n",
        "  * `bh`, `by` – Biases\n",
        "\n",
        "* **Forward Pass**\n",
        "  Computes the hidden state for each time step:\n",
        "  `h_t = tanh(Wxh * x_t + Whh * h_(t-1) + bh)`\n",
        "  Output is then calculated as:\n",
        "  `output = Why * h_t + by`\n",
        "\n",
        "* **Backward Pass (BPTT)**\n",
        "  Gradients are propagated backward through time to update all weights and biases.\n",
        "\n",
        "* **Input Example**\n",
        "  Uses one-hot encoded vectors to represent a sequence.\n",
        "\n",
        "## What This Demonstrates\n",
        "\n",
        "* Memory through hidden states\n",
        "* Step-by-step sequence processing\n",
        "* Manual forward and backward propagation\n",
        "* Gradient-based learning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tI_ycmy80zsN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K5EyE82RtJi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5088d925-e937-4b9e-f027-9185f5a5dbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7.23036265e-05]\n",
            " [-2.94302126e-05]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, input_size, output_size, hidden_size=64):\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Innitialize Weights\n",
        "    self.Wxh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "    self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "    self.Why = np.random.randn(output_size, hidden_size) * 0.01 # Corrected dimension\n",
        "\n",
        "    # Innitalize Biases\n",
        "    self.bh = np.zeros((hidden_size, 1)) # Corrected dimension\n",
        "    self.by = np.zeros((output_size, 1)) # Corrected dimension\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    h_prev = np.zeros((self.hidden_size, 1))\n",
        "    self.last_inputs = inputs\n",
        "    self.last_hs = {0: h_prev}\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    for i, x in enumerate(inputs):\n",
        "      x = np.reshape(x, (self.input_size, 1))\n",
        "      h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_prev) + self.bh)\n",
        "      self.last_hs[i+1] = h\n",
        "      h_prev = h\n",
        "\n",
        "    # Compute the output\n",
        "    output = np.dot(self.Why, h_prev) + self.by\n",
        "    return output, h_prev\n",
        "\n",
        "  def backward(self, d_y, learning_rate=0.01): # Moved inside the class and added d_y as parameter\n",
        "    n = len(self.last_inputs)\n",
        "    d_Why = np.zeros_like(self.Why)\n",
        "    d_Whh = np.zeros_like(self.Whh)\n",
        "    d_Wxh = np.zeros_like(self.Wxh)\n",
        "    d_bh = np.zeros_like(self.bh)\n",
        "    d_by = np.zeros_like(self.by)\n",
        "    d_h = np.dot(self.Why.T, d_y) # Initial gradient from output layer\n",
        "\n",
        "    for t in reversed(range(n)):\n",
        "      temp = self.last_hs[t+1]\n",
        "      d_Why += np.dot(d_y, temp.T)\n",
        "      d_h_raw = (1 - self.last_hs[t+1] ** 2) * d_h # Gradient through tanh\n",
        "\n",
        "      d_bh += d_h_raw\n",
        "      d_Wxh += np.dot(d_h_raw, self.last_inputs[t].reshape(-1, 1).T) # Corrected input shape for dot product\n",
        "      d_Whh += np.dot(d_h_raw, self.last_hs[t].T)\n",
        "      d_h = np.dot(self.Wxh.T, d_h_raw) + np.dot(self.Whh.T, d_h_raw)\n",
        "\n",
        "    # Update Weights and Biases\n",
        "    self.Why -= learning_rate * d_Why\n",
        "    self.Wxh -= learning_rate * d_Wxh\n",
        "    self.Whh -= learning_rate * d_Whh\n",
        "    self.bh -= learning_rate * d_bh\n",
        "    self.by -= learning_rate * d_by\n",
        "\n",
        "\n",
        "# Example sequence (One hot encoded input)\n",
        "input = [np.array([1,0,0]), np.array([0,1,0]), np.array([0,0,1])]\n",
        "rnn = RNN(input_size=3, hidden_size=3, output_size=2)\n",
        "\n",
        "# Forward Pass\n",
        "output, hidden_state = rnn.forward(input)\n",
        "print(output)\n",
        "\n",
        "# Backward Pass\n",
        "d_y = np.array([[0],[1]])\n",
        "rnn.backward(d_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "* The RNN successfully processed the input sequence and produced an **output vector**:\n",
        "\n",
        "```\n",
        "[[ 7.23036265e-05]\n",
        " [-2.94302126e-05]]\n",
        "```\n",
        "\n",
        "* This output shows the network’s initial response to the given sequence.\n",
        "* With **training and multiple iterations**, the RNN can learn patterns in sequential data and produce meaningful predictions.\n",
        "* The experiment demonstrates the **forward and backward propagation** of an RNN and how hidden states carry information across time steps.\n",
        "\n",
        "**Key takeaway:** Even a simple RNN from scratch can model temporal dependencies and serve as a foundation for understanding more advanced sequence models like LSTMs or GRUs.\n"
      ],
      "metadata": {
        "id": "s9nMcTpX289b"
      }
    }
  ]
}